# ğŸ”µ Social Insight - Microservices Architecture

**Architecture**: 3 Independent Microservices  
**Status**: âœ… Production Ready  
**Setup Time**: 10-20 minutes  

---

## ğŸ“Š Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         LOAD BALANCER                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
        â”‚  Data Service â”‚              â”‚  Processing Service â”‚
        â”‚ (Infrastructure)â”‚              â”‚ (Crawlers + Consumer)â”‚
        â”‚                â”‚              â”‚                    â”‚
        â”‚ â€¢ PostgreSQL   â”‚â—„â”€â”€connectionâ”€â”€â”¤ â€¢ HN Crawler       â”‚
        â”‚ â€¢ Redis        â”‚   (env vars)  â”‚ â€¢ DevTo Crawler    â”‚
        â”‚ â€¢ Kafka        â”‚              â”‚ â€¢ Medium Crawler   â”‚
        â”‚ â€¢ Zookeeper    â”‚              â”‚ â€¢ Consumer         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚                              â”‚
                  â”‚â—„â”€â”€â”€â”€â”€connection (env vars)â”€â”€â”¤
                  â”‚                              â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚   API Service     â”‚
                      â”‚                   â”‚
                      â”‚ â€¢ REST API        â”‚
                      â”‚ â€¢ Web Dashboard   â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                        http://localhost:8888
```

---

## ğŸš€ Quick Start (3 Services, 3 Commands)

### Step 1: Start Data Service (Infrastructure)
```bash
cd services/data-service
docker-compose up -d
```

**Expected Output:**
```
Creating data_postgres ... done
Creating data_zookeeper ... done
Creating data_kafka ... done
Creating data_redis ... done
Creating data_kafka_ui ... done
```

**Verify:**
- Kafka UI: http://localhost:8080
- Redis: Check with `redis-cli ping`
- PostgreSQL: Port 5432 ready
- Kafka: Port 9092 ready

---

### Step 2: Start Processing Service (Crawlers + Consumer)
```bash
cd services/processing-service
docker-compose up -d
```

**Expected Output:**
```
Building hn-crawler
Building devto-crawler
Building medium-crawler
Building consumer
Creating processing_hn_crawler ... done
Creating processing_devto_crawler ... done
Creating processing_medium_crawler ... done
Creating processing_consumer ... done
```

**Monitor:**
```bash
docker-compose logs -f
```

You should see:
- Crawlers fetching data from HN, DevTo, Medium
- Consumer reading from Kafka
- Data being saved to PostgreSQL

---

### Step 3: Start API Service (REST API + Dashboard)
```bash
cd services/api-service
docker-compose up -d
```

**Expected Output:**
```
Building api
Creating api_server ... done
```

**Access Dashboard:**
- ğŸŒ http://localhost:8888
- ğŸ“Š API: http://localhost:8888/api/stats
- ğŸ¥ Health: http://localhost:8888/api/health

---

## ğŸ“ Project Structure

```
services/
â”‚
â”œâ”€â”€ data-service/                 # Data Layer (Infrastructure)
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ .env                      # Configuration
â”‚   â”œâ”€â”€ .env.example
â”‚   â”œâ”€â”€ config/                   # Shared config package
â”‚   â”œâ”€â”€ internal/                 # Shared Go packages
â”‚   â”œâ”€â”€ migrations/               # Database migrations
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ processing-service/           # Processing Layer (Crawlers + Consumer)
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ .env                      # Configuration
â”‚   â”œâ”€â”€ .env.example
â”‚   â”œâ”€â”€ Dockerfile.hn-crawler     # HackerNews crawler
â”‚   â”œâ”€â”€ Dockerfile.devto-crawler  # DevTo crawler
â”‚   â”œâ”€â”€ Dockerfile.medium-crawler # Medium crawler
â”‚   â”œâ”€â”€ Dockerfile.consumer       # Kafka consumer
â”‚   â”œâ”€â”€ config/                   # Shared config package
â”‚   â”œâ”€â”€ internal/                 # Shared Go packages
â”‚   â”œâ”€â”€ cmd/
â”‚   â”‚   â”œâ”€â”€ consumer/             # Kafka consumer source
â”‚   â”‚   â””â”€â”€ crawlers/
â”‚   â”‚       â”œâ”€â”€ hn/               # HN crawler source
â”‚   â”‚       â”œâ”€â”€ devto/            # DevTo crawler source
â”‚   â”‚       â””â”€â”€ medium/           # Medium crawler source
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ api-service/                  # API Layer (REST API + Web)
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ .env                      # Configuration
â”‚   â”œâ”€â”€ .env.example
â”‚   â”œâ”€â”€ Dockerfile.api            # API server
â”‚   â”œâ”€â”€ config/                   # Shared config package
â”‚   â”œâ”€â”€ internal/                 # Shared Go packages
â”‚   â”œâ”€â”€ cmd/
â”‚   â”‚   â””â”€â”€ api/                  # API server source
â”‚   â”œâ”€â”€ web/                      # HTML/CSS/JS dashboard
â”‚   â”‚   â””â”€â”€ index.html
â”‚   â””â”€â”€ README.md
â”‚
â””â”€â”€ README.md                     # This file
```

---

## ğŸ”Œ Service Communication

All services communicate through **environment variables**:

### Data Service â†’ Processing Service
```env
KAFKA_HOST=kafka:29092
REDIS_HOST=redis:6379
PG_HOST=postgres
```

### Data Service â†’ API Service
```env
REDIS_HOST=redis:6379
PG_HOST=postgres
```

**No hardcoded values - all configurable via .env files**

---

## ğŸ› ï¸ Commands Reference

### View Logs
```bash
# Data service
cd services/data-service && docker-compose logs -f

# Processing service
cd services/processing-service && docker-compose logs -f

# API service  
cd services/api-service && docker-compose logs -f
```

### Stop Services
```bash
cd services/data-service && docker-compose down
cd services/processing-service && docker-compose down
cd services/api-service && docker-compose down
```

### Remove All Data
```bash
cd services/data-service && docker-compose down -v
cd services/processing-service && docker-compose down -v
cd services/api-service && docker-compose down -v
```

### Rebuild Images
```bash
cd services/processing-service && docker-compose build --no-cache
cd services/api-service && docker-compose build --no-cache
```

### Check Container Status
```bash
docker ps -a | grep data_
docker ps -a | grep processing_
docker ps -a | grep api_
```

---

## ğŸ“Š API Endpoints

### Health Check
```bash
curl http://localhost:8888/api/health
```

### Overall Statistics
```bash
curl http://localhost:8888/api/stats
```

### Statistics by Topic
```bash
curl http://localhost:8888/api/topics
```

### Statistics by Sentiment
```bash
curl http://localhost:8888/api/sentiment
```

### Top Authors
```bash
curl http://localhost:8888/api/authors
```

### Recent Posts
```bash
curl http://localhost:8888/api/recent
```

---

## ğŸ§ª Testing Data Flow

### 1. Verify Data Service is Ready
```bash
# Check PostgreSQL
psql -h localhost -U postgres -d social_insight -c "SELECT COUNT(*) FROM posts;"

# Check Redis
redis-cli PING

# Check Kafka
docker exec data_kafka kafka-topics --bootstrap-server localhost:9092 --list
```

### 2. Monitor Processing Service
```bash
cd services/processing-service
docker-compose logs -f | grep -E "(crawler|consumer)"
```

You should see:
- Crawlers sending posts to Kafka
- Consumer processing messages
- Data saved to PostgreSQL

### 3. Verify API Service
```bash
curl http://localhost:8888/api/health
# Expected: {"status":"ok","time":"2024-01-28T..."}

curl http://localhost:8888/api/stats
# Expected: Shows statistics in JSON format
```

---

## âš™ï¸ Configuration

Each service has its own `.env` file. Copy `.env.example` to `.env` and customize:

### Data Service (.env)
```env
DB_USER=postgres
DB_PASSWORD=postgres123
DB_PORT=5432

KAFKA_PORT=9092
REDIS_PORT=6379
ZOOKEEPER_PORT=2181
```

### Processing Service (.env)
```env
KAFKA_HOST=kafka:29092
REDIS_HOST=redis:6379
PG_HOST=postgres

HN_CRAWL_INTERVAL=5m
DEVTO_CRAWL_INTERVAL=10m
MEDIUM_CRAWL_INTERVAL=10m
```

### API Service (.env)
```env
API_PORT=:8888
REDIS_HOST=redis:6379
PG_HOST=postgres
```

---

## ğŸ› Troubleshooting

### Containers Won't Start
```bash
# Check logs
docker-compose logs -f

# Verify ports are available
netstat -an | grep LISTEN

# Check Docker resources
docker system df
```

### Connection Refused
- Ensure Data Service is running first
- Check environment variables match between services
- Verify network: `docker network ls`

### Kafka Connectivity Issues
```bash
# Test Kafka from host
docker exec data_kafka kafka-topics --bootstrap-server localhost:9092 --list

# Test from another container
docker exec processing_consumer nc -zv kafka 29092
```

### PostgreSQL Connection Failed
```bash
# Test PostgreSQL
psql -h localhost -U postgres -d social_insight

# Check PostgreSQL logs
docker logs data_postgres
```

### Redis Connection Failed
```bash
# Test Redis
redis-cli -h localhost ping

# Check Redis logs
docker logs data_redis
```

---

## ğŸ“ Development Workflow

### Adding New Crawler
1. Create `cmd/crawlers/new-crawler/main.go` in processing-service
2. Add `Dockerfile.new-crawler`
3. Update `docker-compose.yml` with new service
4. Update environment variables in `.env`

### Adding New API Endpoint
1. Edit `cmd/api/main.go` in api-service
2. Test locally: `go run cmd/api/main.go`
3. Rebuild Docker image: `docker build -f Dockerfile.api -t api .`

### Database Schema Changes
1. Create new migration in `data-service/migrations/`
2. Migration runs automatically on Docker startup
3. Update models in `internal/models/` if needed

---

## ğŸ” Security Notes

### For Production:
1. Change PostgreSQL password
2. Change Redis password (add AUTH)
3. Use environment secrets management (Vault, AWS Secrets Manager)
4. Enable HTTPS for API
5. Use private Docker registry
6. Enable PostgreSQL SSL connections
7. Use network policies/firewall rules

### Current Setup (Development Only):
- Default credentials: `postgres:postgres123`
- No authentication on Redis/Kafka
- HTTP only (no HTTPS)
- Open networks between containers

---

## ğŸ“š Related Documentation

- [Data Service README](./services/data-service/README.md)
- [Processing Service README](./services/processing-service/README.md)
- [API Service README](./services/api-service/README.md)

---

## ğŸ¤ Contributing

When modifying services:
1. Never commit `.env` files (use `.env.example`)
2. Update environment variable documentation
3. Test all 3 services together before committing
4. Update this README if adding new features

---

## ğŸ“ Support

For issues or questions:
1. Check individual service README files
2. Review Docker logs: `docker-compose logs -f`
3. Verify environment variables match `.env.example`
4. Ensure services are started in correct order: Data â†’ Processing â†’ API

---

**Last Updated**: January 28, 2025  
**Architecture Version**: 2.0 (Microservices)  
**Go Version**: 1.21+  
**Docker Version**: 20.10+
