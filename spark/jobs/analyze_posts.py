# =====================================================
# SPARK STREAMING JOB - PhÃ¢n tÃ­ch realtime
# =====================================================
# MÃ´ táº£: Spark job cháº¡y LIÃŠN Tá»¤C, phÃ¢n tÃ­ch dá»¯ liá»‡u má»—i 30 giÃ¢y
# Mode: Micro-batch streaming (Ä‘á»c PostgreSQL Ä‘á»‹nh ká»³)
#
# CÃ¡ch cháº¡y: python spark/jobs/analyze_posts.py
# Dá»«ng: Ctrl+C
# =====================================================

from datetime import datetime
import time
import json
import signal
import sys

# Flag Ä‘á»ƒ dá»«ng gracefully
running = True

def signal_handler(sig, frame):
    global running
    print("\nâš ï¸  Nháº­n tÃ­n hiá»‡u dá»«ng, Ä‘ang shutdown...")
    running = False

signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

# =====================================================
# Cáº¤U HÃŒNH
# =====================================================

# PostgreSQL config
PG_CONFIG = {
    "host": "localhost",
    "port": "5432",
    "database": "social_insight",
    "user": "postgres",
    "password": "postgres123"
}

# Streaming config
BATCH_INTERVAL = 30  # Xá»­ lÃ½ má»—i 30 giÃ¢y
OUTPUT_DIR = "analytics_output"

# =====================================================
# DATABASE CONNECTION
# =====================================================

import psycopg2
from psycopg2.extras import RealDictCursor

def get_db_connection():
    """Táº¡o káº¿t ná»‘i database"""
    return psycopg2.connect(
        host=PG_CONFIG["host"],
        port=PG_CONFIG["port"],
        database=PG_CONFIG["database"],
        user=PG_CONFIG["user"],
        password=PG_CONFIG["password"]
    )

# =====================================================
# ANALYTICS FUNCTIONS
# =====================================================

def get_overall_stats(conn):
    """Láº¥y thá»‘ng kÃª tá»•ng quan"""
    with conn.cursor(cursor_factory=RealDictCursor) as cur:
        cur.execute("""
            SELECT 
                COUNT(*) as total_posts,
                COALESCE(SUM(likes), 0) as total_likes,
                COALESCE(SUM(comments), 0) as total_comments,
                COALESCE(AVG(likes), 0) as avg_likes
            FROM posts
        """)
        return dict(cur.fetchone())

def get_topic_stats(conn):
    """Thá»‘ng kÃª theo topic"""
    with conn.cursor(cursor_factory=RealDictCursor) as cur:
        cur.execute("""
            SELECT topic, COUNT(*) as count, SUM(likes) as total_likes
            FROM posts
            GROUP BY topic
            ORDER BY count DESC
        """)
        return {row['topic']: {'count': row['count'], 'likes': row['total_likes']} for row in cur.fetchall()}

def get_sentiment_stats(conn):
    """Thá»‘ng kÃª theo sentiment"""
    with conn.cursor(cursor_factory=RealDictCursor) as cur:
        cur.execute("""
            SELECT sentiment, COUNT(*) as count
            FROM posts
            GROUP BY sentiment
        """)
        total = sum(row['count'] for row in cur.fetchall())
        
        cur.execute("""
            SELECT sentiment, COUNT(*) as count
            FROM posts
            GROUP BY sentiment
        """)
        return {
            row['sentiment']: {
                'count': row['count'],
                'percentage': round(row['count'] / total * 100, 1) if total > 0 else 0
            }
            for row in cur.fetchall()
        }

def get_platform_stats(conn):
    """Thá»‘ng kÃª theo platform"""
    with conn.cursor(cursor_factory=RealDictCursor) as cur:
        cur.execute("""
            SELECT platform, COUNT(*) as count, SUM(likes) as total_likes
            FROM posts
            GROUP BY platform
            ORDER BY count DESC
        """)
        return {row['platform']: {'count': row['count'], 'likes': row['total_likes']} for row in cur.fetchall()}

def get_recent_trend(conn, minutes=5):
    """Trend trong N phÃºt gáº§n nháº¥t"""
    with conn.cursor(cursor_factory=RealDictCursor) as cur:
        cur.execute("""
            SELECT 
                COUNT(*) as posts_count,
                COALESCE(SUM(likes), 0) as total_likes,
                COALESCE(AVG(likes), 0) as avg_likes
            FROM posts
            WHERE created_at >= NOW() - INTERVAL '%s minutes'
        """, (minutes,))
        return dict(cur.fetchone())

def save_analytics(analytics, batch_num):
    """LÆ°u káº¿t quáº£ phÃ¢n tÃ­ch ra JSON"""
    import os
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    filename = f"{OUTPUT_DIR}/analytics_{batch_num:06d}.json"
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(analytics, f, indent=2, ensure_ascii=False)
    
    return filename

# =====================================================
# STREAMING LOOP
# =====================================================

def run_streaming():
    """Main streaming loop"""
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘     SOCIAL INSIGHT - SPARK STREAMING ANALYTICS            â•‘")
    print("â•‘     PhÃ¢n tÃ­ch realtime má»—i 30 giÃ¢y                        â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print()
    print(f"ğŸ“Š Batch interval: {BATCH_INTERVAL} giÃ¢y")
    print(f"ğŸ“ Output directory: {OUTPUT_DIR}/")
    print("   Nháº¥n Ctrl+C Ä‘á»ƒ dá»«ng")
    print()
    print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    
    batch_num = 0
    start_time = datetime.now()
    
    while running:
        batch_num += 1
        batch_start = datetime.now()
        
        try:
            # Káº¿t ná»‘i database
            conn = get_db_connection()
            
            # Thu tháº­p analytics
            analytics = {
                "batch_number": batch_num,
                "timestamp": datetime.now().isoformat(),
                "overall": get_overall_stats(conn),
                "by_topic": get_topic_stats(conn),
                "by_sentiment": get_sentiment_stats(conn),
                "by_platform": get_platform_stats(conn),
                "recent_5min": get_recent_trend(conn, 5)
            }
            
            conn.close()
            
            # LÆ°u káº¿t quáº£
            filename = save_analytics(analytics, batch_num)
            
            # In summary
            total = analytics['overall']['total_posts']
            recent = analytics['recent_5min']['posts_count']
            
            print(f"\nğŸ“ˆ [Batch {batch_num}] {batch_start.strftime('%H:%M:%S')}")
            print(f"   Total posts: {total:,}")
            print(f"   Recent 5min: {recent:,} posts")
            
            if analytics['by_sentiment']:
                sentiments = analytics['by_sentiment']
                pos = sentiments.get('positive', {}).get('percentage', 0)
                neg = sentiments.get('negative', {}).get('percentage', 0)
                print(f"   Sentiment: ğŸ˜Š {pos}% | ğŸ˜¢ {neg}%")
            
            print(f"   Saved: {filename}")
            
        except Exception as e:
            print(f"âŒ [Batch {batch_num}] Error: {e}")
        
        # Äá»£i Ä‘áº¿n batch tiáº¿p theo
        elapsed = (datetime.now() - batch_start).total_seconds()
        sleep_time = max(0, BATCH_INTERVAL - elapsed)
        
        if running and sleep_time > 0:
            time.sleep(sleep_time)
    
    # Káº¿t thÃºc
    total_time = datetime.now() - start_time
    print()
    print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print(f"âœ… Streaming káº¿t thÃºc sau {batch_num} batches")
    print(f"   Thá»i gian cháº¡y: {total_time}")
    print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

# =====================================================
# MAIN
# =====================================================

if __name__ == "__main__":
    try:
        # Test connection trÆ°á»›c
        print("ğŸ“¡ Kiá»ƒm tra káº¿t ná»‘i PostgreSQL...")
        conn = get_db_connection()
        conn.close()
        print("âœ… Káº¿t ná»‘i thÃ nh cÃ´ng!\n")
        
        run_streaming()
        
    except psycopg2.OperationalError as e:
        print(f"âŒ KhÃ´ng thá»ƒ káº¿t ná»‘i PostgreSQL: {e}")
        print("   HÃ£y cháº¯c cháº¯n PostgreSQL Ä‘ang cháº¡y: docker-compose up -d")
        sys.exit(1)
